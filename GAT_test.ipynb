{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I20TFmem9S6B"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo5260KQ98mI",
        "outputId": "f7c6f09c-04f9-42e5-bfb5-d1bff2fc3d58"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c4/ce24841375cf4393787dbf9a645e271c19a03d2d9a0e5770b08ba76bcfde/dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgFgAfpF9V7W",
        "outputId": "83f0d9eb-d818-4383-e8e4-1091cc33284e"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as tg\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch import GATConv\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.g = g\n",
        "        # equation (1)\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        # equation (2)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        # edge UDF for equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        # message UDF for equation (3) & (4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # reduce UDF for equation (3) & (4)\n",
        "        # equation (3)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        # equation (1)\n",
        "        z = self.fc(h)\n",
        "        self.g.ndata['z'] = z\n",
        "        # equation (2)\n",
        "        self.g.apply_edges(self.edge_attention)\n",
        "        # equation (3) & (4)\n",
        "        self.g.update_all(self.message_func, self.reduce_func)\n",
        "        return self.g.ndata.pop('h')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbEVeF0O-Vza"
      },
      "source": [
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            return torch.cat(head_outs, dim=1)\n",
        "        else:\n",
        "            # merge using average\n",
        "            return torch.mean(torch.stack(head_outs))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz4nM6FC-RVv"
      },
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
        "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
        "        # multiple head outputs are concatenated together. Also, only\n",
        "        # one attention head in the output layer.\n",
        "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.layer1(h)\n",
        "        h = F.elu(h)\n",
        "        h = self.layer2(h)\n",
        "        return h"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAAgU7hT9gyQ"
      },
      "source": [
        "from dgl import DGLGraph\n",
        "from dgl.data import citation_graph as citegrh\n",
        "import networkx as nx\n",
        "\n",
        "def load_cora_data():\n",
        "    data = citegrh.load_cora()\n",
        "    features = torch.FloatTensor(data.features)\n",
        "    labels = torch.LongTensor(data.labels)\n",
        "    mask = torch.BoolTensor(data.train_mask)\n",
        "    g = DGLGraph(data.graph)\n",
        "    return g, features, labels, mask, data.test_mask"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX0X6oGY_mU2",
        "outputId": "d4ce34f4-6dcb-4d91-8318-258e03d0bb8b"
      },
      "source": [
        "data.test_mask"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds-lT064963o",
        "outputId": "7683aaf4-eb02-4af4-f991-25ceb3c2d2ad"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "g, features, labels, mask, test_mask = load_cora_data()\n",
        "\n",
        "# create the model, 2 heads, each head has hidden size 8\n",
        "net = GAT(g,\n",
        "          in_dim=features.size()[1],\n",
        "          hidden_dim=8,\n",
        "          out_dim=7,\n",
        "          num_heads=2)\n",
        "\n",
        "# create optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# main loop\n",
        "dur = []\n",
        "for epoch in range(500):\n",
        "    if epoch >= 3:\n",
        "        t0 = time.time()\n",
        "\n",
        "    logits = net(features)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[mask], labels[mask])\n",
        "    pred = logp.argmax(1)\n",
        "\n",
        "    train_acc = (pred[mask] == labels[mask]).float().mean()\n",
        "    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch >= 3:\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "    print(\"Epoch {:05d} | Loss {:.4f} | Training Accuracy {:.4f} | Testing Accuracy {:.4f} | Time(s) {:.4f}\".format(\n",
        "        epoch, loss.item(), train_acc.item(), test_acc.item(), np.mean(dur)))\n",
        "    \n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Epoch 00000 | Loss 1.9463 | Training Accuracy 0.1357 | Testing Accuracy 0.0940 | Time(s) nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.feat will be deprecated, please use g.ndata['feat'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.label will be deprecated, please use g.ndata['label'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.train_mask will be deprecated, please use g.ndata['train_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.test_mask will be deprecated, please use g.ndata['test_mask'] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 00001 | Loss 1.9443 | Training Accuracy 0.2071 | Testing Accuracy 0.1230 | Time(s) nan\n",
            "Epoch 00002 | Loss 1.9423 | Training Accuracy 0.2571 | Testing Accuracy 0.1440 | Time(s) nan\n",
            "Epoch 00003 | Loss 1.9403 | Training Accuracy 0.3071 | Testing Accuracy 0.1650 | Time(s) 0.1290\n",
            "Epoch 00004 | Loss 1.9383 | Training Accuracy 0.3500 | Testing Accuracy 0.1960 | Time(s) 0.1234\n",
            "Epoch 00005 | Loss 1.9363 | Training Accuracy 0.3857 | Testing Accuracy 0.2170 | Time(s) 0.1243\n",
            "Epoch 00006 | Loss 1.9343 | Training Accuracy 0.4357 | Testing Accuracy 0.2530 | Time(s) 0.1237\n",
            "Epoch 00007 | Loss 1.9323 | Training Accuracy 0.4929 | Testing Accuracy 0.2850 | Time(s) 0.1244\n",
            "Epoch 00008 | Loss 1.9303 | Training Accuracy 0.5571 | Testing Accuracy 0.3180 | Time(s) 0.1259\n",
            "Epoch 00009 | Loss 1.9283 | Training Accuracy 0.6429 | Testing Accuracy 0.3450 | Time(s) 0.1259\n",
            "Epoch 00010 | Loss 1.9262 | Training Accuracy 0.7071 | Testing Accuracy 0.3820 | Time(s) 0.1255\n",
            "Epoch 00011 | Loss 1.9242 | Training Accuracy 0.7429 | Testing Accuracy 0.4060 | Time(s) 0.1247\n",
            "Epoch 00012 | Loss 1.9222 | Training Accuracy 0.7571 | Testing Accuracy 0.4270 | Time(s) 0.1239\n",
            "Epoch 00013 | Loss 1.9201 | Training Accuracy 0.8000 | Testing Accuracy 0.4480 | Time(s) 0.1235\n",
            "Epoch 00014 | Loss 1.9181 | Training Accuracy 0.8071 | Testing Accuracy 0.4650 | Time(s) 0.1227\n",
            "Epoch 00015 | Loss 1.9160 | Training Accuracy 0.8500 | Testing Accuracy 0.4890 | Time(s) 0.1227\n",
            "Epoch 00016 | Loss 1.9140 | Training Accuracy 0.8643 | Testing Accuracy 0.5110 | Time(s) 0.1227\n",
            "Epoch 00017 | Loss 1.9119 | Training Accuracy 0.8714 | Testing Accuracy 0.5250 | Time(s) 0.1225\n",
            "Epoch 00018 | Loss 1.9098 | Training Accuracy 0.8929 | Testing Accuracy 0.5500 | Time(s) 0.1222\n",
            "Epoch 00019 | Loss 1.9077 | Training Accuracy 0.9000 | Testing Accuracy 0.5640 | Time(s) 0.1225\n",
            "Epoch 00020 | Loss 1.9056 | Training Accuracy 0.9000 | Testing Accuracy 0.5760 | Time(s) 0.1223\n",
            "Epoch 00021 | Loss 1.9035 | Training Accuracy 0.9000 | Testing Accuracy 0.5810 | Time(s) 0.1221\n",
            "Epoch 00022 | Loss 1.9014 | Training Accuracy 0.9143 | Testing Accuracy 0.5940 | Time(s) 0.1219\n",
            "Epoch 00023 | Loss 1.8992 | Training Accuracy 0.9143 | Testing Accuracy 0.6030 | Time(s) 0.1220\n",
            "Epoch 00024 | Loss 1.8971 | Training Accuracy 0.9286 | Testing Accuracy 0.6090 | Time(s) 0.1217\n",
            "Epoch 00025 | Loss 1.8949 | Training Accuracy 0.9286 | Testing Accuracy 0.6170 | Time(s) 0.1219\n",
            "Epoch 00026 | Loss 1.8927 | Training Accuracy 0.9357 | Testing Accuracy 0.6300 | Time(s) 0.1217\n",
            "Epoch 00027 | Loss 1.8906 | Training Accuracy 0.9357 | Testing Accuracy 0.6400 | Time(s) 0.1217\n",
            "Epoch 00028 | Loss 1.8884 | Training Accuracy 0.9429 | Testing Accuracy 0.6440 | Time(s) 0.1216\n",
            "Epoch 00029 | Loss 1.8861 | Training Accuracy 0.9357 | Testing Accuracy 0.6490 | Time(s) 0.1215\n",
            "Epoch 00030 | Loss 1.8839 | Training Accuracy 0.9357 | Testing Accuracy 0.6560 | Time(s) 0.1215\n",
            "Epoch 00031 | Loss 1.8817 | Training Accuracy 0.9357 | Testing Accuracy 0.6600 | Time(s) 0.1215\n",
            "Epoch 00032 | Loss 1.8794 | Training Accuracy 0.9357 | Testing Accuracy 0.6610 | Time(s) 0.1213\n",
            "Epoch 00033 | Loss 1.8772 | Training Accuracy 0.9357 | Testing Accuracy 0.6630 | Time(s) 0.1218\n",
            "Epoch 00034 | Loss 1.8749 | Training Accuracy 0.9357 | Testing Accuracy 0.6690 | Time(s) 0.1217\n",
            "Epoch 00035 | Loss 1.8726 | Training Accuracy 0.9357 | Testing Accuracy 0.6740 | Time(s) 0.1218\n",
            "Epoch 00036 | Loss 1.8703 | Training Accuracy 0.9357 | Testing Accuracy 0.6780 | Time(s) 0.1217\n",
            "Epoch 00037 | Loss 1.8679 | Training Accuracy 0.9357 | Testing Accuracy 0.6810 | Time(s) 0.1217\n",
            "Epoch 00038 | Loss 1.8656 | Training Accuracy 0.9429 | Testing Accuracy 0.6850 | Time(s) 0.1217\n",
            "Epoch 00039 | Loss 1.8633 | Training Accuracy 0.9429 | Testing Accuracy 0.6850 | Time(s) 0.1217\n",
            "Epoch 00040 | Loss 1.8609 | Training Accuracy 0.9429 | Testing Accuracy 0.6870 | Time(s) 0.1218\n",
            "Epoch 00041 | Loss 1.8585 | Training Accuracy 0.9429 | Testing Accuracy 0.6930 | Time(s) 0.1219\n",
            "Epoch 00042 | Loss 1.8561 | Training Accuracy 0.9429 | Testing Accuracy 0.6940 | Time(s) 0.1219\n",
            "Epoch 00043 | Loss 1.8537 | Training Accuracy 0.9429 | Testing Accuracy 0.6990 | Time(s) 0.1219\n",
            "Epoch 00044 | Loss 1.8512 | Training Accuracy 0.9429 | Testing Accuracy 0.7020 | Time(s) 0.1218\n",
            "Epoch 00045 | Loss 1.8488 | Training Accuracy 0.9429 | Testing Accuracy 0.7020 | Time(s) 0.1219\n",
            "Epoch 00046 | Loss 1.8463 | Training Accuracy 0.9429 | Testing Accuracy 0.7040 | Time(s) 0.1218\n",
            "Epoch 00047 | Loss 1.8438 | Training Accuracy 0.9429 | Testing Accuracy 0.7060 | Time(s) 0.1218\n",
            "Epoch 00048 | Loss 1.8413 | Training Accuracy 0.9429 | Testing Accuracy 0.7100 | Time(s) 0.1217\n",
            "Epoch 00049 | Loss 1.8388 | Training Accuracy 0.9429 | Testing Accuracy 0.7120 | Time(s) 0.1218\n",
            "Epoch 00050 | Loss 1.8363 | Training Accuracy 0.9429 | Testing Accuracy 0.7120 | Time(s) 0.1218\n",
            "Epoch 00051 | Loss 1.8337 | Training Accuracy 0.9429 | Testing Accuracy 0.7150 | Time(s) 0.1218\n",
            "Epoch 00052 | Loss 1.8312 | Training Accuracy 0.9500 | Testing Accuracy 0.7160 | Time(s) 0.1218\n",
            "Epoch 00053 | Loss 1.8286 | Training Accuracy 0.9500 | Testing Accuracy 0.7160 | Time(s) 0.1219\n",
            "Epoch 00054 | Loss 1.8260 | Training Accuracy 0.9500 | Testing Accuracy 0.7190 | Time(s) 0.1219\n",
            "Epoch 00055 | Loss 1.8234 | Training Accuracy 0.9500 | Testing Accuracy 0.7200 | Time(s) 0.1219\n",
            "Epoch 00056 | Loss 1.8207 | Training Accuracy 0.9500 | Testing Accuracy 0.7230 | Time(s) 0.1218\n",
            "Epoch 00057 | Loss 1.8181 | Training Accuracy 0.9500 | Testing Accuracy 0.7250 | Time(s) 0.1218\n",
            "Epoch 00058 | Loss 1.8154 | Training Accuracy 0.9500 | Testing Accuracy 0.7280 | Time(s) 0.1218\n",
            "Epoch 00059 | Loss 1.8127 | Training Accuracy 0.9500 | Testing Accuracy 0.7300 | Time(s) 0.1217\n",
            "Epoch 00060 | Loss 1.8100 | Training Accuracy 0.9500 | Testing Accuracy 0.7300 | Time(s) 0.1217\n",
            "Epoch 00061 | Loss 1.8073 | Training Accuracy 0.9500 | Testing Accuracy 0.7290 | Time(s) 0.1216\n",
            "Epoch 00062 | Loss 1.8046 | Training Accuracy 0.9500 | Testing Accuracy 0.7310 | Time(s) 0.1215\n",
            "Epoch 00063 | Loss 1.8018 | Training Accuracy 0.9500 | Testing Accuracy 0.7310 | Time(s) 0.1215\n",
            "Epoch 00064 | Loss 1.7991 | Training Accuracy 0.9500 | Testing Accuracy 0.7320 | Time(s) 0.1214\n",
            "Epoch 00065 | Loss 1.7963 | Training Accuracy 0.9571 | Testing Accuracy 0.7350 | Time(s) 0.1214\n",
            "Epoch 00066 | Loss 1.7935 | Training Accuracy 0.9571 | Testing Accuracy 0.7350 | Time(s) 0.1214\n",
            "Epoch 00067 | Loss 1.7906 | Training Accuracy 0.9571 | Testing Accuracy 0.7360 | Time(s) 0.1216\n",
            "Epoch 00068 | Loss 1.7878 | Training Accuracy 0.9571 | Testing Accuracy 0.7350 | Time(s) 0.1216\n",
            "Epoch 00069 | Loss 1.7849 | Training Accuracy 0.9571 | Testing Accuracy 0.7360 | Time(s) 0.1215\n",
            "Epoch 00070 | Loss 1.7820 | Training Accuracy 0.9571 | Testing Accuracy 0.7350 | Time(s) 0.1216\n",
            "Epoch 00071 | Loss 1.7791 | Training Accuracy 0.9571 | Testing Accuracy 0.7340 | Time(s) 0.1216\n",
            "Epoch 00072 | Loss 1.7762 | Training Accuracy 0.9571 | Testing Accuracy 0.7340 | Time(s) 0.1216\n",
            "Epoch 00073 | Loss 1.7733 | Training Accuracy 0.9571 | Testing Accuracy 0.7330 | Time(s) 0.1216\n",
            "Epoch 00074 | Loss 1.7704 | Training Accuracy 0.9571 | Testing Accuracy 0.7340 | Time(s) 0.1215\n",
            "Epoch 00075 | Loss 1.7674 | Training Accuracy 0.9571 | Testing Accuracy 0.7350 | Time(s) 0.1216\n",
            "Epoch 00076 | Loss 1.7644 | Training Accuracy 0.9571 | Testing Accuracy 0.7360 | Time(s) 0.1215\n",
            "Epoch 00077 | Loss 1.7614 | Training Accuracy 0.9571 | Testing Accuracy 0.7380 | Time(s) 0.1216\n",
            "Epoch 00078 | Loss 1.7584 | Training Accuracy 0.9571 | Testing Accuracy 0.7390 | Time(s) 0.1217\n",
            "Epoch 00079 | Loss 1.7553 | Training Accuracy 0.9571 | Testing Accuracy 0.7390 | Time(s) 0.1216\n",
            "Epoch 00080 | Loss 1.7523 | Training Accuracy 0.9571 | Testing Accuracy 0.7400 | Time(s) 0.1216\n",
            "Epoch 00081 | Loss 1.7492 | Training Accuracy 0.9571 | Testing Accuracy 0.7400 | Time(s) 0.1215\n",
            "Epoch 00082 | Loss 1.7461 | Training Accuracy 0.9571 | Testing Accuracy 0.7410 | Time(s) 0.1215\n",
            "Epoch 00083 | Loss 1.7430 | Training Accuracy 0.9571 | Testing Accuracy 0.7410 | Time(s) 0.1215\n",
            "Epoch 00084 | Loss 1.7399 | Training Accuracy 0.9571 | Testing Accuracy 0.7410 | Time(s) 0.1215\n",
            "Epoch 00085 | Loss 1.7367 | Training Accuracy 0.9571 | Testing Accuracy 0.7420 | Time(s) 0.1214\n",
            "Epoch 00086 | Loss 1.7335 | Training Accuracy 0.9571 | Testing Accuracy 0.7420 | Time(s) 0.1214\n",
            "Epoch 00087 | Loss 1.7304 | Training Accuracy 0.9571 | Testing Accuracy 0.7420 | Time(s) 0.1214\n",
            "Epoch 00088 | Loss 1.7272 | Training Accuracy 0.9571 | Testing Accuracy 0.7420 | Time(s) 0.1215\n",
            "Epoch 00089 | Loss 1.7239 | Training Accuracy 0.9571 | Testing Accuracy 0.7430 | Time(s) 0.1215\n",
            "Epoch 00090 | Loss 1.7207 | Training Accuracy 0.9571 | Testing Accuracy 0.7430 | Time(s) 0.1215\n",
            "Epoch 00091 | Loss 1.7174 | Training Accuracy 0.9571 | Testing Accuracy 0.7430 | Time(s) 0.1215\n",
            "Epoch 00092 | Loss 1.7142 | Training Accuracy 0.9571 | Testing Accuracy 0.7440 | Time(s) 0.1216\n",
            "Epoch 00093 | Loss 1.7109 | Training Accuracy 0.9571 | Testing Accuracy 0.7430 | Time(s) 0.1216\n",
            "Epoch 00094 | Loss 1.7075 | Training Accuracy 0.9643 | Testing Accuracy 0.7440 | Time(s) 0.1216\n",
            "Epoch 00095 | Loss 1.7042 | Training Accuracy 0.9643 | Testing Accuracy 0.7440 | Time(s) 0.1216\n",
            "Epoch 00096 | Loss 1.7009 | Training Accuracy 0.9714 | Testing Accuracy 0.7440 | Time(s) 0.1215\n",
            "Epoch 00097 | Loss 1.6975 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00098 | Loss 1.6941 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1216\n",
            "Epoch 00099 | Loss 1.6907 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00100 | Loss 1.6873 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1216\n",
            "Epoch 00101 | Loss 1.6839 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00102 | Loss 1.6804 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1216\n",
            "Epoch 00103 | Loss 1.6769 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00104 | Loss 1.6734 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00105 | Loss 1.6699 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00106 | Loss 1.6664 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00107 | Loss 1.6628 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00108 | Loss 1.6593 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00109 | Loss 1.6557 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00110 | Loss 1.6521 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00111 | Loss 1.6485 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00112 | Loss 1.6448 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00113 | Loss 1.6412 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1214\n",
            "Epoch 00114 | Loss 1.6375 | Training Accuracy 0.9714 | Testing Accuracy 0.7490 | Time(s) 0.1214\n",
            "Epoch 00115 | Loss 1.6338 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1214\n",
            "Epoch 00116 | Loss 1.6301 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00117 | Loss 1.6264 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00118 | Loss 1.6226 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1214\n",
            "Epoch 00119 | Loss 1.6189 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00120 | Loss 1.6151 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1215\n",
            "Epoch 00121 | Loss 1.6113 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00122 | Loss 1.6075 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00123 | Loss 1.6037 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1215\n",
            "Epoch 00124 | Loss 1.5998 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1214\n",
            "Epoch 00125 | Loss 1.5960 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00126 | Loss 1.5921 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00127 | Loss 1.5882 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00128 | Loss 1.5843 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00129 | Loss 1.5803 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00130 | Loss 1.5764 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1216\n",
            "Epoch 00131 | Loss 1.5724 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1216\n",
            "Epoch 00132 | Loss 1.5684 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1215\n",
            "Epoch 00133 | Loss 1.5644 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1216\n",
            "Epoch 00134 | Loss 1.5604 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1216\n",
            "Epoch 00135 | Loss 1.5564 | Training Accuracy 0.9714 | Testing Accuracy 0.7470 | Time(s) 0.1216\n",
            "Epoch 00136 | Loss 1.5523 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1216\n",
            "Epoch 00137 | Loss 1.5483 | Training Accuracy 0.9714 | Testing Accuracy 0.7450 | Time(s) 0.1216\n",
            "Epoch 00138 | Loss 1.5442 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1216\n",
            "Epoch 00139 | Loss 1.5401 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1216\n",
            "Epoch 00140 | Loss 1.5360 | Training Accuracy 0.9714 | Testing Accuracy 0.7460 | Time(s) 0.1216\n",
            "Epoch 00141 | Loss 1.5318 | Training Accuracy 0.9714 | Testing Accuracy 0.7480 | Time(s) 0.1216\n",
            "Epoch 00142 | Loss 1.5277 | Training Accuracy 0.9714 | Testing Accuracy 0.7490 | Time(s) 0.1216\n",
            "Epoch 00143 | Loss 1.5235 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00144 | Loss 1.5194 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00145 | Loss 1.5152 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00146 | Loss 1.5109 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00147 | Loss 1.5067 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00148 | Loss 1.5025 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00149 | Loss 1.4982 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00150 | Loss 1.4939 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00151 | Loss 1.4896 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00152 | Loss 1.4853 | Training Accuracy 0.9714 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00153 | Loss 1.4810 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00154 | Loss 1.4767 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00155 | Loss 1.4723 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00156 | Loss 1.4679 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1216\n",
            "Epoch 00157 | Loss 1.4636 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1216\n",
            "Epoch 00158 | Loss 1.4592 | Training Accuracy 0.9786 | Testing Accuracy 0.7480 | Time(s) 0.1216\n",
            "Epoch 00159 | Loss 1.4547 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00160 | Loss 1.4503 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1216\n",
            "Epoch 00161 | Loss 1.4458 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00162 | Loss 1.4414 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00163 | Loss 1.4369 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00164 | Loss 1.4324 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00165 | Loss 1.4279 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00166 | Loss 1.4234 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00167 | Loss 1.4188 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00168 | Loss 1.4143 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00169 | Loss 1.4097 | Training Accuracy 0.9786 | Testing Accuracy 0.7500 | Time(s) 0.1217\n",
            "Epoch 00170 | Loss 1.4051 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00171 | Loss 1.4005 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00172 | Loss 1.3959 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00173 | Loss 1.3912 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00174 | Loss 1.3866 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1217\n",
            "Epoch 00175 | Loss 1.3819 | Training Accuracy 0.9786 | Testing Accuracy 0.7490 | Time(s) 0.1218\n",
            "Epoch 00176 | Loss 1.3773 | Training Accuracy 0.9786 | Testing Accuracy 0.7480 | Time(s) 0.1218\n",
            "Epoch 00177 | Loss 1.3726 | Training Accuracy 0.9786 | Testing Accuracy 0.7470 | Time(s) 0.1217\n",
            "Epoch 00178 | Loss 1.3679 | Training Accuracy 0.9786 | Testing Accuracy 0.7470 | Time(s) 0.1217\n",
            "Epoch 00179 | Loss 1.3632 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00180 | Loss 1.3584 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00181 | Loss 1.3537 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00182 | Loss 1.3489 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00183 | Loss 1.3442 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00184 | Loss 1.3394 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00185 | Loss 1.3346 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00186 | Loss 1.3298 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00187 | Loss 1.3250 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00188 | Loss 1.3202 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00189 | Loss 1.3153 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1217\n",
            "Epoch 00190 | Loss 1.3105 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1216\n",
            "Epoch 00191 | Loss 1.3056 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1217\n",
            "Epoch 00192 | Loss 1.3008 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00193 | Loss 1.2959 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00194 | Loss 1.2910 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00195 | Loss 1.2861 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00196 | Loss 1.2812 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00197 | Loss 1.2763 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1217\n",
            "Epoch 00198 | Loss 1.2713 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1217\n",
            "Epoch 00199 | Loss 1.2664 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1218\n",
            "Epoch 00200 | Loss 1.2615 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1218\n",
            "Epoch 00201 | Loss 1.2565 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00202 | Loss 1.2515 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00203 | Loss 1.2466 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1218\n",
            "Epoch 00204 | Loss 1.2416 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1217\n",
            "Epoch 00205 | Loss 1.2366 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1217\n",
            "Epoch 00206 | Loss 1.2316 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00207 | Loss 1.2266 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00208 | Loss 1.2216 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00209 | Loss 1.2166 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1217\n",
            "Epoch 00210 | Loss 1.2116 | Training Accuracy 0.9786 | Testing Accuracy 0.7460 | Time(s) 0.1218\n",
            "Epoch 00211 | Loss 1.2065 | Training Accuracy 0.9786 | Testing Accuracy 0.7450 | Time(s) 0.1218\n",
            "Epoch 00212 | Loss 1.2015 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1218\n",
            "Epoch 00213 | Loss 1.1965 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00214 | Loss 1.1914 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00215 | Loss 1.1864 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00216 | Loss 1.1813 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1218\n",
            "Epoch 00217 | Loss 1.1763 | Training Accuracy 0.9786 | Testing Accuracy 0.7440 | Time(s) 0.1218\n",
            "Epoch 00218 | Loss 1.1712 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00219 | Loss 1.1662 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00220 | Loss 1.1611 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00221 | Loss 1.1561 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00222 | Loss 1.1510 | Training Accuracy 0.9786 | Testing Accuracy 0.7420 | Time(s) 0.1218\n",
            "Epoch 00223 | Loss 1.1459 | Training Accuracy 0.9786 | Testing Accuracy 0.7420 | Time(s) 0.1218\n",
            "Epoch 00224 | Loss 1.1409 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00225 | Loss 1.1358 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00226 | Loss 1.1308 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00227 | Loss 1.1257 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00228 | Loss 1.1206 | Training Accuracy 0.9786 | Testing Accuracy 0.7430 | Time(s) 0.1218\n",
            "Epoch 00229 | Loss 1.1156 | Training Accuracy 0.9857 | Testing Accuracy 0.7420 | Time(s) 0.1218\n",
            "Epoch 00230 | Loss 1.1105 | Training Accuracy 0.9857 | Testing Accuracy 0.7420 | Time(s) 0.1218\n",
            "Epoch 00231 | Loss 1.1055 | Training Accuracy 0.9857 | Testing Accuracy 0.7410 | Time(s) 0.1218\n",
            "Epoch 00232 | Loss 1.1004 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00233 | Loss 1.0954 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00234 | Loss 1.0903 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00235 | Loss 1.0853 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00236 | Loss 1.0803 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00237 | Loss 1.0752 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00238 | Loss 1.0702 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00239 | Loss 1.0652 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00240 | Loss 1.0602 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00241 | Loss 1.0552 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00242 | Loss 1.0502 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00243 | Loss 1.0452 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00244 | Loss 1.0402 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00245 | Loss 1.0352 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00246 | Loss 1.0303 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00247 | Loss 1.0253 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00248 | Loss 1.0204 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00249 | Loss 1.0154 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00250 | Loss 1.0105 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00251 | Loss 1.0056 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00252 | Loss 1.0006 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00253 | Loss 0.9957 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00254 | Loss 0.9908 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00255 | Loss 0.9859 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00256 | Loss 0.9811 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00257 | Loss 0.9762 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1219\n",
            "Epoch 00258 | Loss 0.9713 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00259 | Loss 0.9665 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00260 | Loss 0.9617 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1219\n",
            "Epoch 00261 | Loss 0.9569 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00262 | Loss 0.9520 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00263 | Loss 0.9472 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00264 | Loss 0.9425 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00265 | Loss 0.9377 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00266 | Loss 0.9329 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00267 | Loss 0.9282 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00268 | Loss 0.9235 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00269 | Loss 0.9187 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00270 | Loss 0.9140 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00271 | Loss 0.9093 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00272 | Loss 0.9047 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00273 | Loss 0.9000 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00274 | Loss 0.8954 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00275 | Loss 0.8907 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00276 | Loss 0.8861 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00277 | Loss 0.8815 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00278 | Loss 0.8769 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00279 | Loss 0.8723 | Training Accuracy 0.9857 | Testing Accuracy 0.7410 | Time(s) 0.1218\n",
            "Epoch 00280 | Loss 0.8678 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00281 | Loss 0.8632 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00282 | Loss 0.8587 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00283 | Loss 0.8542 | Training Accuracy 0.9857 | Testing Accuracy 0.7400 | Time(s) 0.1218\n",
            "Epoch 00284 | Loss 0.8497 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00285 | Loss 0.8452 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00286 | Loss 0.8407 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1218\n",
            "Epoch 00287 | Loss 0.8363 | Training Accuracy 0.9857 | Testing Accuracy 0.7390 | Time(s) 0.1219\n",
            "Epoch 00288 | Loss 0.8319 | Training Accuracy 0.9857 | Testing Accuracy 0.7380 | Time(s) 0.1218\n",
            "Epoch 00289 | Loss 0.8274 | Training Accuracy 0.9857 | Testing Accuracy 0.7380 | Time(s) 0.1219\n",
            "Epoch 00290 | Loss 0.8231 | Training Accuracy 0.9857 | Testing Accuracy 0.7370 | Time(s) 0.1219\n",
            "Epoch 00291 | Loss 0.8187 | Training Accuracy 0.9857 | Testing Accuracy 0.7370 | Time(s) 0.1219\n",
            "Epoch 00292 | Loss 0.8143 | Training Accuracy 0.9857 | Testing Accuracy 0.7370 | Time(s) 0.1219\n",
            "Epoch 00293 | Loss 0.8100 | Training Accuracy 0.9857 | Testing Accuracy 0.7380 | Time(s) 0.1219\n",
            "Epoch 00294 | Loss 0.8056 | Training Accuracy 0.9857 | Testing Accuracy 0.7380 | Time(s) 0.1219\n",
            "Epoch 00295 | Loss 0.8013 | Training Accuracy 0.9857 | Testing Accuracy 0.7370 | Time(s) 0.1219\n",
            "Epoch 00296 | Loss 0.7970 | Training Accuracy 0.9857 | Testing Accuracy 0.7370 | Time(s) 0.1219\n",
            "Epoch 00297 | Loss 0.7927 | Training Accuracy 0.9857 | Testing Accuracy 0.7360 | Time(s) 0.1219\n",
            "Epoch 00298 | Loss 0.7885 | Training Accuracy 0.9857 | Testing Accuracy 0.7350 | Time(s) 0.1219\n",
            "Epoch 00299 | Loss 0.7842 | Training Accuracy 0.9857 | Testing Accuracy 0.7360 | Time(s) 0.1219\n",
            "Epoch 00300 | Loss 0.7800 | Training Accuracy 0.9857 | Testing Accuracy 0.7360 | Time(s) 0.1219\n",
            "Epoch 00301 | Loss 0.7758 | Training Accuracy 0.9857 | Testing Accuracy 0.7360 | Time(s) 0.1219\n",
            "Epoch 00302 | Loss 0.7716 | Training Accuracy 0.9857 | Testing Accuracy 0.7360 | Time(s) 0.1219\n",
            "Epoch 00303 | Loss 0.7674 | Training Accuracy 0.9857 | Testing Accuracy 0.7350 | Time(s) 0.1219\n",
            "Epoch 00304 | Loss 0.7633 | Training Accuracy 0.9857 | Testing Accuracy 0.7350 | Time(s) 0.1219\n",
            "Epoch 00305 | Loss 0.7591 | Training Accuracy 0.9857 | Testing Accuracy 0.7340 | Time(s) 0.1219\n",
            "Epoch 00306 | Loss 0.7550 | Training Accuracy 0.9857 | Testing Accuracy 0.7330 | Time(s) 0.1219\n",
            "Epoch 00307 | Loss 0.7509 | Training Accuracy 0.9857 | Testing Accuracy 0.7330 | Time(s) 0.1219\n",
            "Epoch 00308 | Loss 0.7468 | Training Accuracy 0.9857 | Testing Accuracy 0.7330 | Time(s) 0.1219\n",
            "Epoch 00309 | Loss 0.7427 | Training Accuracy 0.9857 | Testing Accuracy 0.7330 | Time(s) 0.1219\n",
            "Epoch 00310 | Loss 0.7387 | Training Accuracy 0.9857 | Testing Accuracy 0.7340 | Time(s) 0.1219\n",
            "Epoch 00311 | Loss 0.7347 | Training Accuracy 0.9857 | Testing Accuracy 0.7340 | Time(s) 0.1219\n",
            "Epoch 00312 | Loss 0.7306 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1219\n",
            "Epoch 00313 | Loss 0.7266 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1219\n",
            "Epoch 00314 | Loss 0.7227 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1219\n",
            "Epoch 00315 | Loss 0.7187 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1219\n",
            "Epoch 00316 | Loss 0.7147 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00317 | Loss 0.7108 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00318 | Loss 0.7069 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00319 | Loss 0.7030 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1219\n",
            "Epoch 00320 | Loss 0.6991 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1219\n",
            "Epoch 00321 | Loss 0.6953 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1219\n",
            "Epoch 00322 | Loss 0.6914 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1219\n",
            "Epoch 00323 | Loss 0.6876 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1219\n",
            "Epoch 00324 | Loss 0.6838 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00325 | Loss 0.6800 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00326 | Loss 0.6763 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00327 | Loss 0.6725 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00328 | Loss 0.6688 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00329 | Loss 0.6651 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00330 | Loss 0.6614 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00331 | Loss 0.6577 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00332 | Loss 0.6540 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1220\n",
            "Epoch 00333 | Loss 0.6504 | Training Accuracy 0.9857 | Testing Accuracy 0.7320 | Time(s) 0.1220\n",
            "Epoch 00334 | Loss 0.6467 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00335 | Loss 0.6431 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00336 | Loss 0.6395 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00337 | Loss 0.6360 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1220\n",
            "Epoch 00338 | Loss 0.6324 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00339 | Loss 0.6289 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00340 | Loss 0.6253 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00341 | Loss 0.6218 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00342 | Loss 0.6183 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1220\n",
            "Epoch 00343 | Loss 0.6149 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1221\n",
            "Epoch 00344 | Loss 0.6114 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1221\n",
            "Epoch 00345 | Loss 0.6080 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1221\n",
            "Epoch 00346 | Loss 0.6046 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1221\n",
            "Epoch 00347 | Loss 0.6011 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1221\n",
            "Epoch 00348 | Loss 0.5978 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1221\n",
            "Epoch 00349 | Loss 0.5944 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1221\n",
            "Epoch 00350 | Loss 0.5910 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00351 | Loss 0.5877 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00352 | Loss 0.5844 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00353 | Loss 0.5811 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00354 | Loss 0.5778 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00355 | Loss 0.5745 | Training Accuracy 0.9857 | Testing Accuracy 0.7310 | Time(s) 0.1221\n",
            "Epoch 00356 | Loss 0.5713 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00357 | Loss 0.5680 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00358 | Loss 0.5648 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00359 | Loss 0.5616 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00360 | Loss 0.5584 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00361 | Loss 0.5552 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00362 | Loss 0.5521 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1222\n",
            "Epoch 00363 | Loss 0.5489 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1222\n",
            "Epoch 00364 | Loss 0.5458 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1222\n",
            "Epoch 00365 | Loss 0.5427 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1223\n",
            "Epoch 00366 | Loss 0.5396 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1223\n",
            "Epoch 00367 | Loss 0.5366 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1223\n",
            "Epoch 00368 | Loss 0.5335 | Training Accuracy 0.9786 | Testing Accuracy 0.7290 | Time(s) 0.1223\n",
            "Epoch 00369 | Loss 0.5305 | Training Accuracy 0.9786 | Testing Accuracy 0.7290 | Time(s) 0.1223\n",
            "Epoch 00370 | Loss 0.5275 | Training Accuracy 0.9786 | Testing Accuracy 0.7290 | Time(s) 0.1223\n",
            "Epoch 00371 | Loss 0.5245 | Training Accuracy 0.9786 | Testing Accuracy 0.7290 | Time(s) 0.1223\n",
            "Epoch 00372 | Loss 0.5215 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00373 | Loss 0.5185 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00374 | Loss 0.5155 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00375 | Loss 0.5126 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00376 | Loss 0.5097 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00377 | Loss 0.5068 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00378 | Loss 0.5039 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00379 | Loss 0.5010 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00380 | Loss 0.4981 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00381 | Loss 0.4953 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1223\n",
            "Epoch 00382 | Loss 0.4925 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1223\n",
            "Epoch 00383 | Loss 0.4897 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1224\n",
            "Epoch 00384 | Loss 0.4869 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1224\n",
            "Epoch 00385 | Loss 0.4841 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1224\n",
            "Epoch 00386 | Loss 0.4813 | Training Accuracy 0.9786 | Testing Accuracy 0.7280 | Time(s) 0.1224\n",
            "Epoch 00387 | Loss 0.4786 | Training Accuracy 0.9786 | Testing Accuracy 0.7280 | Time(s) 0.1224\n",
            "Epoch 00388 | Loss 0.4758 | Training Accuracy 0.9786 | Testing Accuracy 0.7270 | Time(s) 0.1224\n",
            "Epoch 00389 | Loss 0.4731 | Training Accuracy 0.9786 | Testing Accuracy 0.7270 | Time(s) 0.1224\n",
            "Epoch 00390 | Loss 0.4704 | Training Accuracy 0.9786 | Testing Accuracy 0.7270 | Time(s) 0.1224\n",
            "Epoch 00391 | Loss 0.4677 | Training Accuracy 0.9786 | Testing Accuracy 0.7290 | Time(s) 0.1224\n",
            "Epoch 00392 | Loss 0.4650 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1224\n",
            "Epoch 00393 | Loss 0.4624 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1225\n",
            "Epoch 00394 | Loss 0.4597 | Training Accuracy 0.9786 | Testing Accuracy 0.7310 | Time(s) 0.1225\n",
            "Epoch 00395 | Loss 0.4571 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00396 | Loss 0.4545 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00397 | Loss 0.4519 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00398 | Loss 0.4493 | Training Accuracy 0.9786 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00399 | Loss 0.4467 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00400 | Loss 0.4441 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1225\n",
            "Epoch 00401 | Loss 0.4416 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00402 | Loss 0.4391 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1226\n",
            "Epoch 00403 | Loss 0.4365 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00404 | Loss 0.4340 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00405 | Loss 0.4315 | Training Accuracy 0.9857 | Testing Accuracy 0.7280 | Time(s) 0.1226\n",
            "Epoch 00406 | Loss 0.4290 | Training Accuracy 0.9857 | Testing Accuracy 0.7280 | Time(s) 0.1226\n",
            "Epoch 00407 | Loss 0.4265 | Training Accuracy 0.9857 | Testing Accuracy 0.7280 | Time(s) 0.1226\n",
            "Epoch 00408 | Loss 0.4241 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00409 | Loss 0.4216 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00410 | Loss 0.4192 | Training Accuracy 0.9857 | Testing Accuracy 0.7290 | Time(s) 0.1226\n",
            "Epoch 00411 | Loss 0.4168 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1226\n",
            "Epoch 00412 | Loss 0.4144 | Training Accuracy 0.9857 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00413 | Loss 0.4120 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00414 | Loss 0.4096 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00415 | Loss 0.4072 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00416 | Loss 0.4048 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00417 | Loss 0.4025 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00418 | Loss 0.4002 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00419 | Loss 0.3978 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00420 | Loss 0.3955 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00421 | Loss 0.3932 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00422 | Loss 0.3910 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00423 | Loss 0.3887 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00424 | Loss 0.3864 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00425 | Loss 0.3842 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00426 | Loss 0.3820 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00427 | Loss 0.3797 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00428 | Loss 0.3775 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00429 | Loss 0.3754 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00430 | Loss 0.3732 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00431 | Loss 0.3710 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00432 | Loss 0.3689 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00433 | Loss 0.3667 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00434 | Loss 0.3646 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00435 | Loss 0.3625 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00436 | Loss 0.3604 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00437 | Loss 0.3583 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1228\n",
            "Epoch 00438 | Loss 0.3563 | Training Accuracy 0.9929 | Testing Accuracy 0.7300 | Time(s) 0.1227\n",
            "Epoch 00439 | Loss 0.3542 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00440 | Loss 0.3522 | Training Accuracy 0.9929 | Testing Accuracy 0.7290 | Time(s) 0.1227\n",
            "Epoch 00441 | Loss 0.3502 | Training Accuracy 0.9929 | Testing Accuracy 0.7280 | Time(s) 0.1228\n",
            "Epoch 00442 | Loss 0.3481 | Training Accuracy 0.9929 | Testing Accuracy 0.7280 | Time(s) 0.1228\n",
            "Epoch 00443 | Loss 0.3461 | Training Accuracy 0.9929 | Testing Accuracy 0.7280 | Time(s) 0.1228\n",
            "Epoch 00444 | Loss 0.3441 | Training Accuracy 0.9929 | Testing Accuracy 0.7270 | Time(s) 0.1228\n",
            "Epoch 00445 | Loss 0.3421 | Training Accuracy 0.9929 | Testing Accuracy 0.7270 | Time(s) 0.1228\n",
            "Epoch 00446 | Loss 0.3402 | Training Accuracy 0.9929 | Testing Accuracy 0.7270 | Time(s) 0.1228\n",
            "Epoch 00447 | Loss 0.3382 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00448 | Loss 0.3363 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00449 | Loss 0.3343 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00450 | Loss 0.3324 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00451 | Loss 0.3305 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00452 | Loss 0.3286 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00453 | Loss 0.3267 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00454 | Loss 0.3248 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00455 | Loss 0.3229 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00456 | Loss 0.3211 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00457 | Loss 0.3192 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00458 | Loss 0.3174 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00459 | Loss 0.3156 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00460 | Loss 0.3138 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00461 | Loss 0.3120 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1228\n",
            "Epoch 00462 | Loss 0.3102 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00463 | Loss 0.3084 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00464 | Loss 0.3066 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00465 | Loss 0.3049 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00466 | Loss 0.3031 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00467 | Loss 0.3014 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1228\n",
            "Epoch 00468 | Loss 0.2996 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1228\n",
            "Epoch 00469 | Loss 0.2979 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00470 | Loss 0.2962 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00471 | Loss 0.2945 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00472 | Loss 0.2928 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00473 | Loss 0.2911 | Training Accuracy 0.9929 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00474 | Loss 0.2895 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00475 | Loss 0.2878 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00476 | Loss 0.2862 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00477 | Loss 0.2845 | Training Accuracy 0.9929 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00478 | Loss 0.2829 | Training Accuracy 0.9929 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00479 | Loss 0.2813 | Training Accuracy 1.0000 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00480 | Loss 0.2797 | Training Accuracy 1.0000 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00481 | Loss 0.2781 | Training Accuracy 1.0000 | Testing Accuracy 0.7240 | Time(s) 0.1229\n",
            "Epoch 00482 | Loss 0.2765 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00483 | Loss 0.2749 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00484 | Loss 0.2733 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00485 | Loss 0.2718 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00486 | Loss 0.2702 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00487 | Loss 0.2687 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00488 | Loss 0.2671 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00489 | Loss 0.2656 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00490 | Loss 0.2641 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00491 | Loss 0.2626 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00492 | Loss 0.2611 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00493 | Loss 0.2596 | Training Accuracy 1.0000 | Testing Accuracy 0.7250 | Time(s) 0.1229\n",
            "Epoch 00494 | Loss 0.2581 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00495 | Loss 0.2566 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00496 | Loss 0.2552 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00497 | Loss 0.2537 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00498 | Loss 0.2523 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n",
            "Epoch 00499 | Loss 0.2509 | Training Accuracy 1.0000 | Testing Accuracy 0.7260 | Time(s) 0.1229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inLdUauS_H3r",
        "outputId": "e104291e-5ecb-4727-dd1e-de297f4d2cf8"
      },
      "source": [
        "logp.argmax(1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 4, 4,  ..., 3, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXmm-P38-2_G",
        "outputId": "ebc1a16d-98f0-49d5-def7-bb5d1ea8a62c"
      },
      "source": [
        "logp"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.9590, -1.9811, -1.9442,  ..., -1.9677, -1.9696, -1.9328],\n",
              "        [-1.9500, -1.9678, -1.9415,  ..., -1.8976, -1.9645, -1.9607],\n",
              "        [-1.9388, -2.0051, -1.9410,  ..., -1.8799, -1.9569, -1.9730],\n",
              "        ...,\n",
              "        [-1.9465, -1.9239, -1.9630,  ..., -1.9787, -1.9252, -1.9698],\n",
              "        [-1.9347, -1.9868, -1.9368,  ..., -1.9109, -1.9519, -1.9807],\n",
              "        [-1.9357, -1.9757, -1.9487,  ..., -1.9256, -1.9539, -1.9656]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hec8jr7D-MOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}